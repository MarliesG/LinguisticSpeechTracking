# -*- coding: utf-8 -*-
"""Workshop_Linguistic Speech Tracking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TenZTwYLW5zcbAmsYb7OrKn3XQyaCEov

# Install necessary packages

## Show time (installation can take around 10 minutes)
"""

import time
print(time.strftime('%H:%M:%S'))

"""## Install miniconda and mamba"""

!pip install -q condacolab
import condacolab
condacolab.install()

"""## Install eelbrain"""

import condacolab
condacolab.check()

!pip install praat-textgrids

!mamba install -y eelbrain

"""## Check if installation is successful"""

import eelbrain
print(eelbrain.__version__)

"""## Mount Google Drive"""

# mount google shared folder to your google drive:
# (1) open the link (https://drive.google.com/drive/folders/1v2hXcOYmBGDsneqUXAKO0EINGfmObrE7?usp=sharing)
# (2) click on the small arrow next to 'CNSP_workshop'
# (3) make shortcut to your own Drive
# aka no need to mount the google drive locally

from google.colab import drive
drive.mount('/mnt/drive')

"""## Setup

# **Initialize and add parameters**

### Import packages
"""

import re

import eelbrain
import mne
import numpy as np

# eelbrain
import eelbrain
# see documentation of eelbrain toolbox: https://eelbrain.readthedocs.io

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import textgrids

from pathlib import Path

"""### Data locations"""

# Location of data
DATA_ROOT = '/mnt/drive/MyDrive/CNSP_Tutorial_Track_1'
STIMULI_DIR = DATA_ROOT + '/data/stimuli'
OTHER_DIR = DATA_ROOT + '/data/other'
TRF_DIR = DATA_ROOT + '/data/TRFs'
EEG_DIR = DATA_ROOT + '/data/eeg'

"""# **1_Making speech features**

Crafting the speech features is the key of a good model. Depending on the research questions, different speech features should be used. Here, we are interested in linguistic speech tracking. However, since linguistic features are highly correlated with acoustic speech features, we want to control for acoustic speech processing. Hence, acoustic, (sub-)lexical as well as linguistic speech features should be extracted.

### **Acoustic speech features**
"""

# visualize the speech spectrogram
# the speech spectrogram was created using the gammatone filterbank, this quite computationally heavy, therefore, the spectrogram was precomputed
spectrogram = eelbrain.load.unpickle(os.path.join(STIMULI_DIR, 'audiobook_1_64Hz_binned_spectrogram.pickle'))
p = eelbrain.plot.Array(spectrogram.sub(time=(97, 100)))

# Exercise: Create and Visualize the feature acoustic onsets. Acoustic onsets is a speech feature which stresses the onsets of the acoustics, aka the onsets of the high burst of acoustic energy.
# To calculate acoustic onsets, the spectrogram is derived and halve-wave rectified (aka negative parts are set to zero)
# These manipulation can be easily done using eelbrain, check out the class nd var (https://eelbrain.readthedocs.io/en/stable/generated/eelbrain.NDVar.html -- check out the methods 'diff' and 'clip')
acoustic_onsets = spectrogram.diff('time').clip(0)

p = eelbrain.plot.Array(acoustic_onsets.sub(time=(97,100)))

# Exercise: Check out the relation between spectrogram and acoustic onsets (to do so you can use the mean across frequencies or the lowest frequency band)
p = eelbrain.plot.UTS([[spectrogram.mean('frequency', name='avg_spectrogram').sub(time=(97, 100)), acoustic_onsets.mean('frequency', name='avg_acoustic_onsets').sub(time=(97, 100))]])

"""### **(Sub-)lexical features**"""

# To create the phoneme and word onsets features, we determined the onset of a word or phoneme using the BAS web services (https://clarin.phonetik.uni-muenchen.de/BASWebServices/interface/WebMAUSBasic)
# By uploading the wav file and the transcript, a textgrid is obtained. From this textgrid, we create the phoneme onsets stimuli.
textgrid_file = os.path.join(OTHER_DIR, 'audiobook_1.TextGrid')
fs = 1/spectrogram.time.tstep # sampling rate by preprocessed EEG of sparrkulee dataset

grid = textgrids.TextGrid(textgrid_file)
grid.keys()

grid['ORT-MAU']

word_onset_times = [interval.xmin for interval in grid['ORT-MAU'] if interval.text not in ['']]

time = eelbrain.UTS.from_int(0,  np.ceil(grid.xmax*fs), fs)
stimulus_wordOnsets = eelbrain.NDVar(np.zeros(len(time)), time, name='word onsets')
stimulus_wordOnsets[word_onset_times] = 1

# visualize the word onsets on the spectrogram (averaged across frequency bands)
p = eelbrain.plot.UTS([[spectrogram.mean('frequency', name='avg_spectrogram').sub(time=(97, 100)), stimulus_wordOnsets.sub(time=(97, 100))]])

# Exercise: make the stimulus for phoneme onsets and plot it together with the spectorgram and word onsets on a figure.
# A tip: unknown utterances are indicated by following phonemes: '<p:>', '<p>', '<nib>', '<usb>'
phoneme_onset_times = [interval.xmin for interval in grid['MAU'] if interval.text not in ['<p:>', '<p>', '<nib>', 'usb']]

stimulus_phonemeOnsets = eelbrain.NDVar(np.zeros(len(time)), time, name='phoneme onsets')
stimulus_phonemeOnsets[phoneme_onset_times] = 1

p = eelbrain.plot.UTS([[spectrogram.mean('frequency').sub(time=(97,100)),
                       stimulus_phonemeOnsets.sub(time=(97, 100))*2,
                       stimulus_wordOnsets.sub(time=(97, 100))]])

"""### **Linguistic features**"""

# Linguistic features require language models (or at least word probabilities) and are therefore computationally quite expensive. Therefore, these features were pre-computed.
phoneme_surprisal = eelbrain.load.unpickle(os.path.join(STIMULI_DIR, 'audiobook_1_64Hz_phoneme_surprisal_subtlex.pickle'))
p = eelbrain.plot.UTS(phoneme_surprisal.sub(time=(97, 100)))

# Exercise: plot all linguistic speech features on top of each other
cohort_entropy = eelbrain.load.unpickle(os.path.join(STIMULI_DIR, 'audiobook_1_64Hz_cohort_entropy_subtlex.pickle'))

word_surprisal = eelbrain.load.unpickle(os.path.join(STIMULI_DIR, 'audiobook_1_64Hz_word_surprisal_ngram.pickle'))

word_frequency = eelbrain.load.unpickle(os.path.join(STIMULI_DIR, 'audiobook_1_64Hz_word_frequency_ngram.pickle'))

p = eelbrain.plot.UTS([[phoneme_surprisal.sub(time=(97, 100)),
                        cohort_entropy.sub(time=(97,100))+2,
                        word_surprisal.sub(time=(97, 100))+3,
                        word_frequency.sub(time=(97,100))+5]])

"""# **2_Estimating forward models**"""

# estimate the forward model using the spectrogram and inspect the prediction accuracy (for one subject only)
# get the EEG → make it an eelbrain ND var
eeg_data = np.load(os.path.join(EEG_DIR, 'sub-001_ses-shortstories01_task-listeningActive_run-04_desc-preproc-audio-audiobook_1_eeg.npy'))
sensor = eelbrain.Sensor.from_montage('biosemi64')[:64]
time = eelbrain.UTS(0, 1 / fs, eeg_data.shape[1])
eeg = eelbrain.NDVar(eeg_data.astype('float64'), (sensor, time), name='eeg_data')

eeg = eeg.sub(time=(None, spectrogram.time.tstop))

# estimate the forward model
# → time is reduced to 3min (180s) to speed up the computation time
r = eelbrain.boosting(eeg.sub(time=(None, 180)), spectrogram.sub(time=(None, 180)), tstart=0, tstop=0.5, basis=0.05, partitions=5, test=1, selective_stopping=1)

# visualize the prediction accuracy
p = eelbrain.plot.Topomap(r.r, head_radius=0.45, clip='circle')
c = p.plot_colorbar('Prediction accuracy [Pearson\'s r]')

# visualize the trf
plot = eelbrain.plot.TopoButterfly(r.h.mean('frequency'), t=0.11, ylabel='TRF weights [a.u.]', frame='t', xlabel='time (s)', clip='circle', head_radius=0.45)

# Exercise: check out the effect of setting a different basis kernel. This affects the smoothness of the TRF
# to reduce computation time: reduce to 1 minute and disable the test partition
# visualize the results for a channel selection (see below)
results = []
for basis in [0, 0.05, 0.1, 0.15]:
  r_b = eelbrain.boosting(eeg.sub(time=(0, 60)), spectrogram.sub(time=(0, 60)), tstart=0, tstop=0.5, partitions=5, test=0, basis=basis)
  results.append(r_b)

channel_selection = ['F1', 'Fz', 'F2', 'FC1', 'FCz', 'FC2']
sensors = eeg.get_dim('sensor')
zeros_nd = eelbrain.NDVar(np.zeros(64), (sensors,))

p = eelbrain.plot.Topomap(zeros_nd, head_radius=0.45, clip='circle')
p.mark_sensors(channel_selection, c='r')

p = eelbrain.plot.UTS([[r.h.mean('frequency').sub(sensor=channel_selection).mean('sensor') for r in results]])

# Exercise: adding multiple features (e.g., spectrogram and phoneme onsets) and visualize prediction accuracy and TRF (just 3 minute of data to reduce the computation time and compare this to the r-object
# that we've created earlier)
r_2 = eelbrain.boosting(eeg.sub(time=(0, 180)), [spectrogram.sub(time=(0, 180)), stimulus_phonemeOnsets.sub(time=(0, 180))], basis=0.05, partitions=5, tstart=0, tstop=0.5, selective_stopping=1)

# visualize the prediction accuracy
p = eelbrain.plot.Topomap(r_2.r, head_radius=0.45, clip='circle')
c = p.plot_colorbar('Prediction accuracy [Pearson\'s r]')

# visualize the trf
plot = eelbrain.plot.TopoButterfly(r_2.h[0].mean('frequency'), t=0.11, ylabel='TRF weights [a.u.]', frame='t', xlabel='time (s)', clip='circle', head_radius=0.45, title='spectrogram')
plot = eelbrain.plot.TopoButterfly(r_2.h[1], t=0.2, ylabel='TRF weights [a.u.]', frame='t', xlabel='time (s)', clip='circle', head_radius=0.45, title='phoneme onsets', vmax=0.01, vmin=-0.01)

# Exercise: determine the added value of phoneme onsets on top of the spectrogram
r_added_value = r_2.r - r.r

p=eelbrain.plot.Topomap(r_added_value, head_radius=0.45, clip='circle',vmax=0.1)
c = p.plot_colorbar('Difference in prediction accuracy')

# You can get insights in how the data is split in order to estimate the forward model using: p = eelbrain.plot.preview_partitions()
# specify the amount of partitions using partitions=5; testing partition test=True
p = eelbrain.plot.preview_partitions(partitions=5, test=1)

"""# **3_Added value of linguistic features**"""

# read all the files, save into a eelbrain dataframe
files = os.listdir(TRF_DIR)

rows = []
for filename in files:
  d = eelbrain.load.unpickle(os.path.join(TRF_DIR, filename))

  subject = filename.split('_')[0]
  model = filename.split('_')[1].replace('.pickle', '')

  rows.append([subject, model, d.r])

# make dataframe
ds_prediction_accuracy = eelbrain.Dataset.from_caselist(['subject', 'model', 'prediction_accuracy'], rows, random='subject')

ds_prediction_accuracy

# visualize the prediction accuracy for each model
p = eelbrain.plot.Topomap('prediction_accuracy', 'model', ds=ds_prediction_accuracy, ncol=2, clip='circle', head_radius=0.45)
c = p.plot_colorbar('Prediction accuracy [Pearson\'s r]')

# check if there is a significant difference
res = eelbrain.testnd.TTestRelated('prediction_accuracy', 'model', match='subject', ds=ds_prediction_accuracy)
p = eelbrain.plot.Topomap(res, clip='circle', head_radius=0.45, ncol=3)
c = p.plot_colorbar('Prediction accuracy [Pearson\'s r]')

p = eelbrain.plot.Topomap(res.masked_difference(), clip='circle', head_radius=0.45, ncol=3)
c = p.plot_colorbar('Prediction accuracy [Pearson\'s r]')

# Exercise: added value of multiple linguistic features, to do so, for each subject, take the difference between the prediction accuracy of the complete model and the baseline model
rows_linguistic_tracking = []
for subject in np.unique(ds_prediction_accuracy['subject']):
  complete = ds_prediction_accuracy[np.logical_and(ds_prediction_accuracy['subject']==subject,
                                                   ds_prediction_accuracy['model']=='complete')]
  assert complete.shape[0] == 1

  baseline = ds_prediction_accuracy[np.logical_and(ds_prediction_accuracy['subject']==subject,
                                                   ds_prediction_accuracy['model'] == 'baseline')]
  assert baseline.shape[0] == 1

  added_value = complete['prediction_accuracy']-baseline['prediction_accuracy']
  rows_linguistic_tracking.append([subject, added_value])

ds_linguistic_tracking = eelbrain.Dataset.from_caselist(['subject', 'added_value'], rows_linguistic_tracking, random='subject')

p = eelbrain.plot.Topomap('added_value', ds=ds_linguistic_tracking, head_radius=0.45, clip='circle')

# next up: look at the TRFs of the complete model.
# note: an eelbrain dataframe requires that the features are in the same dimension, therefore, take the mean across frequency bands for the acoustic features
# note: an eelbrain dataframe requires that the features are in trows_trf = []
rows_trf = []
for filename in files:
  d = eelbrain.load.unpickle(os.path.join(TRF_DIR, filename))

  subject = filename.split('_')[0]
  model = filename.split('_')[1].replace('.pickle', '')

  if not model in ['complete']:
    continue

  trfs = d.h
  for trf in trfs:
    if trf.name in ['binned spectrogram', 'acoustic onsets']:
      trf = trf.mean('frequency')
    rows_trf.append([subject, trf, trf.name])

# to dataframe
ds_trf = eelbrain.Dataset.from_caselist(['subject', 'trf', 'feature'], rows_trf, random='subject')

np.unique(ds_trf['feature'])

# visualize the TRF for spectrogram
p = eelbrain.plot.TopoButterfly('trf', ds=ds_trf[ds_trf['feature']=='binned spectrogram'], t=0.1, clip='circle', head_radius=0.45)

# take average across channel selection
trf_spectrogram_channel_selection = ds_trf[ds_trf['feature']=='binned spectrogram', 'trf'].sub(sensor=channel_selection).mean('sensor')

p = eelbrain.plot.UTSStat(trf_spectrogram_channel_selection)

res = eelbrain.testnd.TTestOneSample(ds_trf[ds_trf['feature']=='binned spectrogram', 'trf'], pmin=0.05)
p = eelbrain.plot.Butterfly(res)

# Exercise: visualize the TRFs of the linguistic features.
# Tip: before averaging across a channel selection, inspect the TRF across all channels, it is highly likely that a different
# channel selection needs to be taken
for feature, t in zip(['cohort_entropy_subtlex', 'word_surprisal_ngram'], [0.3, 0.35]):
  p = eelbrain.plot.TopoButterfly('trf', ds=ds_trf[ds_trf['feature']==feature], t=t, title=feature)

